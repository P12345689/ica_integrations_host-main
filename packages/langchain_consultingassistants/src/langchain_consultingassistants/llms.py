# -*- coding: utf-8 -*-
"""
Authors: Chris Hay, Mihai Criveti
Description: IBM Consulting Assistants Langchain Extensions API - Python SDK
"""

from typing import Any, Dict, List, Optional, Union

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import BaseLLM
from langchain_core.outputs import Generation, LLMResult
from libica import ICAClient


class ConsultingAssistantsLLM(BaseLLM):
    """
    A class representing the IBM Consulting Assistants Language Model.

    This class provides methods to interact with the IBM Consulting Assistants
    API, including generating responses based on prompts and formatting the
    results into a standardized LLMResult object.
    """

    @property
    def _llm_type(self) -> str:
        return "IBM Consulting Assistants"

    @property
    def lc_secrets(self) -> Dict[str, str]:
        return {
            "url": "ASSISTANTS_BASE_URL",
            "x-access-token": "ASSISTANTS_ACCESS_TOKEN",
            "x-security-key": "ASSISTANTS_API_KEY",
            "x-extension-app-id": "ASSISTANTS_APP_ID",
        }

    params: Optional[dict] = None
    model: str = ""

    def _get_chat_params(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Get the parameters for the chat interaction.

        Args:
            stop (Optional[List[str]]): A list of stop sequences for the language model.

        Returns:
            Dict[str, Any]: A dictionary of parameters for the chat interaction.
        """
        params: Optional[Dict[str, Any]] = {**self.params} if self.params else None
        if stop is not None:
            params = (params or {}) | {"stop_sequences": stop}
        return params

    def _create_llm_result(self, response: List[dict]) -> LLMResult:
        """
        Create an LLMResult object from the response of the language model.

        Args:
            response (List[dict]): The raw response from the language model.

        Returns:
            LLMResult: The formatted result object.
        """
        # generations
        generations = []

        # go through the responses
        for res in response:
            # get the results
            results = res.get("results")

            if results:
                # get the stop reason
                finish_reason = results[0].get("stop_reason")

                # get the generation
                gen = Generation(
                    # set the text
                    text=results[0].get("generated_text"),
                    # stop reason
                    generation_info={"finish_reason": finish_reason},
                )
                generations.append([gen])

        # llm output
        llm_output = {
            # "token_usage": final_token_usage,
            # "model_id": self.model_id,
        }

        # return the result
        return LLMResult(generations=generations, llm_output=llm_output)

    def _call(
        self,
        prompt: Union[str, Dict[str, str]],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        Call the language model with a given prompt and additional parameters.

        Args:
            prompt (Union[str, Dict[str, str]]): The prompt to send to the language model.
            stop (Optional[List[str]]): A list of stop sequences for the language model.
            run_manager (Optional[CallbackManagerForLLMRun]): A manager for callback functions.
            **kwargs: Additional keyword arguments.

        Returns:
            str: The text generated by the language model.
        """
        if isinstance(prompt, dict):
            prompt = self.prompt_template.format(**prompt)

        result = self._generate(prompts=[prompt], stop=stop, run_manager=run_manager, **kwargs)

        return result.generations[0][0].text

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        stream: Optional[bool] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """
        Generate responses from the language model for a list of prompts.

        Args:
            prompts (List[str]): A list of prompts to send to the language model.
            stop (Optional[List[str]]): A list of stop sequences for the language model.
            run_manager (Optional[CallbackManagerForLLMRun]): A manager for callback functions.
            stream (Optional[bool]): Whether to stream the results.
            **kwargs: Additional keyword arguments.

        Returns:
            LLMResult: The result object containing the generated responses.
        """
        # Get the chat parameters
        params = self._get_chat_params(stop=stop)

        # get the response
        consulting_assistants_model = ICAClient()
        response = consulting_assistants_model.prompt_flow(
            model_id_or_name=self.model, prompt=prompts[0], parameters=params
        )

        # parse the response
        response_dict = {"results": [{"generated_text": response}]}

        # return the response
        response_list = [response_dict]

        # return the result
        return self._create_llm_result(response_list)
